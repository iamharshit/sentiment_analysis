{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MILESTONE 2\n",
    "\n",
    "IMDB dataset + Siraj's Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset\n",
    "\n",
    "1. Removing punctuations\n",
    "2. Generating word_to_int map\n",
    "3. Coverting each review in ints\n",
    "4. Padding each review with 0's and generating input of length 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <PERIOD> ')\n",
    "    text = text.replace('\"', ' <PERIOD> ')\n",
    "    text = text.replace(';', ' <PERIOD> ')\n",
    "    text = text.replace('!', ' <PERIOD> ')\n",
    "    text = text.replace('?', ' <PERIOD> ')\n",
    "    text = text.replace('(', ' <PERIOD> ')\n",
    "    text = text.replace(')', ' <PERIOD> ')\n",
    "    text = text.replace('--', ' <PERIOD> ')\n",
    "    text = text.replace('?', ' <PERIOD> ')\n",
    "    '''\n",
    "    text = text.replace('<br />', ' <PERIOD> ')\n",
    "    text = text.replace('\\\\', ' <PERIOD> ')\n",
    "    text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <PERIOD> ')\n",
    "    text = text.replace(' <PERIOD> ', ' ')\n",
    "    words = text.split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "def removing_noise(words):\n",
    "    word_count = Counter(words)\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    words_new = [word for word in words if (word_count[word]>5) #and (not word in stops)\n",
    "                ]\n",
    "    return words_new\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'data/labeledTrainData.tsv'\n",
    "review_ids = []\n",
    "reviews = []\n",
    "labels = []\n",
    "#importing dataset into lists\n",
    "wrong_temp = []\n",
    "#[92, 102, 120, 259, 404, 1028, 1094, 1184, 1229, 1234, 1343, 1503, 1790, 1861, 2212, 3430, 3771, 3870, 4106, 4407, 4866, 5053, 5218, 5221, 5514, 5553, 5646, 5853, 6086, 6499, 6582, 6746, 7021, 7023, 7194, 7331, 7454, 7473, 7553, 7837, 8119, 8264, 8407, 8433, 8971, 9076, 9204, 9402, 9490, 9552, 9562, 9632, 9716, 9748, 9787, 10107, 10230, 10233, 10414, 10477, 10500, 10702, 10892, 11048, 11055, 11371, 11375, 11513, 11744, 11944, 12071, 12159, 12188, 12243, 12341, 12558, 12594, 12808, 13087, 13159, 14111, 14755, 14860, 14993, 15094, 15260, 15352, 15360, 15656, 15871, 16214, 16274, 16492, 16539, 16613, 16622, 16870, 16949, 16990, 16992, 17071, 17254, 17371, 17450, 17463, 17603, 17709, 17712, 17749, 18024, 18221, 18226, 18681, 18784, 18896, 19063, 19609, 19714, 19889]\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    row_count = 0\n",
    "    for row in reader:\n",
    "        review_ids.append(row[0])\n",
    "        labels.append([int(row[1])] )\n",
    "        reviews.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_pp = []\n",
    "words = []\n",
    "\n",
    "for review in reviews:\n",
    "    review_pp = preprocess(review)\n",
    "    reviews_pp.append(review_pp)\n",
    "    words.extend(review_pp)\n",
    "    \n",
    "words = removing_noise(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting word to integers and making the vocabulary\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "words_count = Counter(words)\n",
    "sorted_vocab = sorted(words_count, key = words_count.get, reverse = True)\n",
    "word_to_int = {word:i for i,word in enumerate(sorted_vocab,1)}\n",
    "\n",
    "#Converting each review in the form of integers\n",
    "reviews_pp_ints = []\n",
    "for review in reviews_pp:\n",
    "    this_review_int = []\n",
    "    for word in review:\n",
    "        if word in vocab:\n",
    "            this_review_int.append(word_to_int[word])\n",
    "    reviews_pp_ints.append(this_review_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp_ints[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_pp_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_len = 200\n",
    "features = np.zeros((len(reviews_pp_ints), max_seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_pp_ints):\n",
    "    features[i, :len(row)] = np.array(row[:max_seq_len] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'features' is a 2d array storing all sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "embed_size = 100\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "hidden_nodes = 10\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'labels')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.set_random_seed(5)\n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform((vocab_size+1, embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "\n",
    "#getting an initial state of zeros\\n\",\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n",
    "\n",
    "#hidden_layer = tf.contrib.layers.fully_connected(outputs[:, -1], hidden_nodes, activation_fn=tf.nn.relu)\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1],1, activation_fn=tf.sigmoid)\n",
    "#predictions,Y\\n\",\n",
    "\n",
    "#regularizers =  tf.reduce_mean(tf.nn.l2_loss(tf.trainable_variables() ))\n",
    "regularizers =sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables() if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name))\n",
    "loss = tf.reduce_mean(tf.square(Y - predictions) )+ 0.01*regularizers\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy:\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.float32), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/60 Iteration: 5 Train loss: 6166.73584\n",
      "Epoch: 0/60 Iteration: 10 Train loss: 5260.27881\n",
      "Epoch: 0/60 Iteration: 15 Train loss: 4458.91406\n",
      "Epoch: 0/60 Iteration: 20 Train loss: 3757.53931\n",
      "Epoch: 0/60 Iteration: 25 Train loss: 3149.47559\n",
      "Epoch: 0/60 Iteration: 30 Train loss: 2626.40991\n",
      "Epoch: 0/60 Iteration: 35 Train loss: 2179.65918\n",
      "Epoch: 0/60 Iteration: 40 Train loss: 1800.54956\n",
      "Epoch: 1/60 Iteration: 5 Train loss: 1480.68298\n",
      "Epoch: 1/60 Iteration: 10 Train loss: 1212.23315\n",
      "Epoch: 1/60 Iteration: 15 Train loss: 988.06384\n",
      "Epoch: 1/60 Iteration: 20 Train loss: 801.76215\n",
      "Epoch: 1/60 Iteration: 25 Train loss: 647.65875\n",
      "Epoch: 1/60 Iteration: 30 Train loss: 520.77972\n",
      "Epoch: 1/60 Iteration: 35 Train loss: 416.80371\n",
      "Epoch: 1/60 Iteration: 40 Train loss: 331.99969\n",
      "Epoch: 2/60 Iteration: 5 Train loss: 263.16507\n",
      "Epoch: 2/60 Iteration: 10 Train loss: 207.56963\n",
      "Epoch: 2/60 Iteration: 15 Train loss: 162.89330\n",
      "Epoch: 2/60 Iteration: 20 Train loss: 127.17708\n",
      "Epoch: 2/60 Iteration: 25 Train loss: 98.77533\n",
      "Epoch: 2/60 Iteration: 30 Train loss: 76.31257\n",
      "Epoch: 2/60 Iteration: 35 Train loss: 58.64507\n",
      "Epoch: 2/60 Iteration: 40 Train loss: 44.82772\n",
      "Epoch: 3/60 Iteration: 5 Train loss: 34.08368\n",
      "Epoch: 3/60 Iteration: 10 Train loss: 25.77866\n",
      "Epoch: 3/60 Iteration: 15 Train loss: 19.39713\n",
      "Epoch: 3/60 Iteration: 20 Train loss: 14.52357\n",
      "Epoch: 3/60 Iteration: 25 Train loss: 10.82459\n",
      "Epoch: 3/60 Iteration: 30 Train loss: 8.03489\n",
      "Epoch: 3/60 Iteration: 35 Train loss: 5.94416\n",
      "Epoch: 3/60 Iteration: 40 Train loss: 4.38771\n",
      "Epoch: 4/60 Iteration: 5 Train loss: 3.23650\n",
      "Epoch: 4/60 Iteration: 10 Train loss: 2.39104\n",
      "Epoch: 4/60 Iteration: 15 Train loss: 1.77414\n",
      "Epoch: 4/60 Iteration: 20 Train loss: 1.32733\n",
      "Epoch: 4/60 Iteration: 25 Train loss: 1.00595\n",
      "Epoch: 4/60 Iteration: 30 Train loss: 0.77658\n",
      "Epoch: 4/60 Iteration: 35 Train loss: 0.61380\n",
      "Epoch: 4/60 Iteration: 40 Train loss: 0.49945\n",
      "Epoch: 5/60 Iteration: 5 Train loss: 0.41953\n",
      "Epoch: 5/60 Iteration: 10 Train loss: 0.36441\n",
      "Epoch: 5/60 Iteration: 15 Train loss: 0.32645\n",
      "Epoch: 5/60 Iteration: 20 Train loss: 0.30071\n",
      "Epoch: 5/60 Iteration: 25 Train loss: 0.28333\n",
      "Epoch: 5/60 Iteration: 30 Train loss: 0.27181\n",
      "Epoch: 5/60 Iteration: 35 Train loss: 0.26393\n",
      "Epoch: 5/60 Iteration: 40 Train loss: 0.25890\n",
      "Epoch: 6/60 Iteration: 5 Train loss: 0.25550\n",
      "Epoch: 6/60 Iteration: 10 Train loss: 0.25350\n",
      "Epoch: 6/60 Iteration: 15 Train loss: 0.25212\n",
      "Epoch: 6/60 Iteration: 20 Train loss: 0.25134\n",
      "Epoch: 6/60 Iteration: 25 Train loss: 0.25085\n",
      "Epoch: 6/60 Iteration: 30 Train loss: 0.25066\n",
      "Epoch: 6/60 Iteration: 35 Train loss: 0.25029\n",
      "Epoch: 6/60 Iteration: 40 Train loss: 0.25020\n",
      "Epoch: 7/60 Iteration: 5 Train loss: 0.25000\n",
      "Epoch: 7/60 Iteration: 10 Train loss: 0.25008\n",
      "Epoch: 7/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 7/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 7/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 7/60 Iteration: 30 Train loss: 0.25021\n",
      "Epoch: 7/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 7/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 8/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 8/60 Iteration: 10 Train loss: 0.25003\n",
      "Epoch: 8/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 8/60 Iteration: 20 Train loss: 0.25005\n",
      "Epoch: 8/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 8/60 Iteration: 30 Train loss: 0.25021\n",
      "Epoch: 8/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 8/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 9/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 9/60 Iteration: 10 Train loss: 0.25003\n",
      "Epoch: 9/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 9/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 9/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 9/60 Iteration: 30 Train loss: 0.25021\n",
      "Epoch: 9/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 9/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 10/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 10/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 10/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 10/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 10/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 10/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 10/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 10/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 11/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 11/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 11/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 11/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 11/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 11/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 11/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 11/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 12/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 12/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 12/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 12/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 12/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 12/60 Iteration: 30 Train loss: 0.25022\n",
      "Epoch: 12/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 12/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 13/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 13/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 13/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 13/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 13/60 Iteration: 25 Train loss: 0.25009\n",
      "Epoch: 13/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 13/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 13/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 14/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 14/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 14/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 14/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 14/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 14/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 14/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 14/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 15/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 15/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 15/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 15/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 15/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 15/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 15/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 15/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 16/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 16/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 16/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 16/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 16/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 16/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 16/60 Iteration: 35 Train loss: 0.25002\n",
      "Epoch: 16/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 17/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 17/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 17/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 17/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 17/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 17/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 17/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 17/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 18/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 18/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 18/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 18/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 18/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 18/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 18/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 18/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 19/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 19/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 19/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 19/60 Iteration: 20 Train loss: 0.25006\n",
      "Epoch: 19/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 19/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 19/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 19/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 20/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 20/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 20/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 20/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 20/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 20/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 20/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 20/60 Iteration: 40 Train loss: 0.25005\n",
      "Epoch: 21/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 21/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 21/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 21/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 21/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 21/60 Iteration: 30 Train loss: 0.25023\n",
      "Epoch: 21/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 21/60 Iteration: 40 Train loss: 0.25006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 22/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 22/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 22/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 22/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 22/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 22/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 22/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 23/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 23/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 23/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 23/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 23/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 23/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 23/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 23/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 24/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 24/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 24/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 24/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 24/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 24/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 24/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 24/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 25/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 25/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 25/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 25/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 25/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 25/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 25/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 25/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 26/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 26/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 26/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 26/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 26/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 26/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 26/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 26/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 27/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 27/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 27/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 27/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 27/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 27/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 27/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 27/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 28/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 28/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 28/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 28/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 28/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 28/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 28/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 28/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 29/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 29/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 29/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 29/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 29/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 29/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 29/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 29/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 30/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 30/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 30/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 30/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 30/60 Iteration: 25 Train loss: 0.25010\n",
      "Epoch: 30/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 30/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 30/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 31/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 31/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 31/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 31/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 31/60 Iteration: 25 Train loss: 0.25011\n",
      "Epoch: 31/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 31/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 31/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 32/60 Iteration: 5 Train loss: 0.24991\n",
      "Epoch: 32/60 Iteration: 10 Train loss: 0.25004\n",
      "Epoch: 32/60 Iteration: 15 Train loss: 0.24999\n",
      "Epoch: 32/60 Iteration: 20 Train loss: 0.25007\n",
      "Epoch: 32/60 Iteration: 25 Train loss: 0.25011\n",
      "Epoch: 32/60 Iteration: 30 Train loss: 0.25024\n",
      "Epoch: 32/60 Iteration: 35 Train loss: 0.25003\n",
      "Epoch: 32/60 Iteration: 40 Train loss: 0.25006\n",
      "Epoch: 33/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 33/60 Iteration: 10 Train loss: 0.25012\n",
      "Epoch: 33/60 Iteration: 15 Train loss: 0.25006\n",
      "Epoch: 33/60 Iteration: 20 Train loss: 0.25013\n",
      "Epoch: 33/60 Iteration: 25 Train loss: 0.25016\n",
      "Epoch: 33/60 Iteration: 30 Train loss: 0.25030\n",
      "Epoch: 33/60 Iteration: 35 Train loss: 0.25008\n",
      "Epoch: 33/60 Iteration: 40 Train loss: 0.25010\n",
      "Epoch: 34/60 Iteration: 5 Train loss: 0.24995\n",
      "Epoch: 34/60 Iteration: 10 Train loss: 0.25008\n",
      "Epoch: 34/60 Iteration: 15 Train loss: 0.25003\n",
      "Epoch: 34/60 Iteration: 20 Train loss: 0.25011\n",
      "Epoch: 34/60 Iteration: 25 Train loss: 0.25014\n",
      "Epoch: 34/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 34/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 34/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 35/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 35/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 35/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 35/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 35/60 Iteration: 25 Train loss: 0.25014\n",
      "Epoch: 35/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 35/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 35/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 36/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 36/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 36/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 36/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 36/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 36/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 36/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 36/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 37/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 37/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 37/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 37/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 37/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 37/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 37/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 37/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 38/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 38/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 38/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 38/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 38/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 38/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 38/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 38/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 39/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 39/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 39/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 39/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 39/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 39/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 39/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 39/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 40/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 40/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 40/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 40/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 40/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 40/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 40/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 40/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 41/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 41/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 41/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 41/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 41/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 41/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 41/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 41/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 42/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 42/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 42/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 42/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 42/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 42/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 42/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 42/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 43/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 43/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 43/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 43/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 43/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 43/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 43/60 Iteration: 35 Train loss: 0.25005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 44/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 44/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 44/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 44/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 44/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 44/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 44/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 44/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 45/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 45/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 45/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 45/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 45/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 45/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 45/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 45/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 46/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 46/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 46/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 46/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 46/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 46/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 46/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 46/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 47/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 47/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 47/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 47/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 47/60 Iteration: 25 Train loss: 0.25014\n",
      "Epoch: 47/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 47/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 47/60 Iteration: 40 Train loss: 0.25009\n",
      "Epoch: 48/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 48/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 48/60 Iteration: 15 Train loss: 0.25002\n",
      "Epoch: 48/60 Iteration: 20 Train loss: 0.25010\n",
      "Epoch: 48/60 Iteration: 25 Train loss: 0.25014\n",
      "Epoch: 48/60 Iteration: 30 Train loss: 0.25028\n",
      "Epoch: 48/60 Iteration: 35 Train loss: 0.25007\n",
      "Epoch: 48/60 Iteration: 40 Train loss: 0.25010\n",
      "Epoch: 49/60 Iteration: 5 Train loss: 0.24994\n",
      "Epoch: 49/60 Iteration: 10 Train loss: 0.25007\n",
      "Epoch: 49/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 49/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 49/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 49/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 49/60 Iteration: 35 Train loss: 0.25005\n",
      "Epoch: 49/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 50/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 50/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 50/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 50/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 50/60 Iteration: 25 Train loss: 0.25012\n",
      "Epoch: 50/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 50/60 Iteration: 35 Train loss: 0.25004\n",
      "Epoch: 50/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 51/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 51/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 51/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 51/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 51/60 Iteration: 25 Train loss: 0.25012\n",
      "Epoch: 51/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 51/60 Iteration: 35 Train loss: 0.25004\n",
      "Epoch: 51/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 52/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 52/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 52/60 Iteration: 15 Train loss: 0.25001\n",
      "Epoch: 52/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 52/60 Iteration: 25 Train loss: 0.25012\n",
      "Epoch: 52/60 Iteration: 30 Train loss: 0.25026\n",
      "Epoch: 52/60 Iteration: 35 Train loss: 0.25004\n",
      "Epoch: 52/60 Iteration: 40 Train loss: 0.25008\n",
      "Epoch: 53/60 Iteration: 5 Train loss: 0.24993\n",
      "Epoch: 53/60 Iteration: 10 Train loss: 0.25006\n",
      "Epoch: 53/60 Iteration: 15 Train loss: 0.25000\n",
      "Epoch: 53/60 Iteration: 20 Train loss: 0.25009\n",
      "Epoch: 53/60 Iteration: 25 Train loss: 0.25013\n",
      "Epoch: 53/60 Iteration: 30 Train loss: 0.25027\n",
      "Epoch: 53/60 Iteration: 35 Train loss: 0.25006\n",
      "Epoch: 53/60 Iteration: 40 Train loss: 0.25010\n",
      "Epoch: 54/60 Iteration: 5 Train loss: 0.24997\n",
      "Epoch: 54/60 Iteration: 10 Train loss: 0.25011\n",
      "Epoch: 54/60 Iteration: 15 Train loss: 0.25008\n",
      "Epoch: 54/60 Iteration: 20 Train loss: 0.25017\n",
      "Epoch: 54/60 Iteration: 25 Train loss: 0.25022\n",
      "Epoch: 54/60 Iteration: 30 Train loss: 0.25036\n",
      "Epoch: 54/60 Iteration: 35 Train loss: 0.25014\n",
      "Epoch: 54/60 Iteration: 40 Train loss: 0.25018\n",
      "Epoch: 55/60 Iteration: 5 Train loss: 0.25002\n",
      "Epoch: 55/60 Iteration: 10 Train loss: 0.25015\n",
      "Epoch: 55/60 Iteration: 15 Train loss: 0.25009\n",
      "Epoch: 55/60 Iteration: 20 Train loss: 0.25016\n",
      "Epoch: 55/60 Iteration: 25 Train loss: 0.25019\n",
      "Epoch: 55/60 Iteration: 30 Train loss: 0.25033\n",
      "Epoch: 55/60 Iteration: 35 Train loss: 0.25011\n",
      "Epoch: 55/60 Iteration: 40 Train loss: 0.25014\n",
      "Epoch: 56/60 Iteration: 5 Train loss: 0.24999\n",
      "Epoch: 56/60 Iteration: 10 Train loss: 0.25013\n",
      "Epoch: 56/60 Iteration: 15 Train loss: 0.25009\n",
      "Epoch: 56/60 Iteration: 20 Train loss: 0.25017\n",
      "Epoch: 56/60 Iteration: 25 Train loss: 0.25019\n",
      "Epoch: 56/60 Iteration: 30 Train loss: 0.25033\n",
      "Epoch: 56/60 Iteration: 35 Train loss: 0.25011\n",
      "Epoch: 56/60 Iteration: 40 Train loss: 0.25014\n",
      "Epoch: 57/60 Iteration: 5 Train loss: 0.24998\n",
      "Epoch: 57/60 Iteration: 10 Train loss: 0.25011\n",
      "Epoch: 57/60 Iteration: 15 Train loss: 0.25005\n",
      "Epoch: 57/60 Iteration: 20 Train loss: 0.25014\n",
      "Epoch: 57/60 Iteration: 25 Train loss: 0.25017\n",
      "Epoch: 57/60 Iteration: 30 Train loss: 0.25030\n",
      "Epoch: 57/60 Iteration: 35 Train loss: 0.25009\n",
      "Epoch: 57/60 Iteration: 40 Train loss: 0.25011\n",
      "Epoch: 58/60 Iteration: 5 Train loss: 0.24997\n",
      "Epoch: 58/60 Iteration: 10 Train loss: 0.25009\n",
      "Epoch: 58/60 Iteration: 15 Train loss: 0.25004\n",
      "Epoch: 58/60 Iteration: 20 Train loss: 0.25012\n",
      "Epoch: 58/60 Iteration: 25 Train loss: 0.25015\n",
      "Epoch: 58/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 58/60 Iteration: 35 Train loss: 0.25007\n",
      "Epoch: 58/60 Iteration: 40 Train loss: 0.25010\n",
      "Epoch: 59/60 Iteration: 5 Train loss: 0.24996\n",
      "Epoch: 59/60 Iteration: 10 Train loss: 0.25009\n",
      "Epoch: 59/60 Iteration: 15 Train loss: 0.25003\n",
      "Epoch: 59/60 Iteration: 20 Train loss: 0.25011\n",
      "Epoch: 59/60 Iteration: 25 Train loss: 0.25015\n",
      "Epoch: 59/60 Iteration: 30 Train loss: 0.25029\n",
      "Epoch: 59/60 Iteration: 35 Train loss: 0.25007\n",
      "Epoch: 59/60 Iteration: 40 Train loss: 0.25010\n",
      "Training Completed\n",
      "Total Time Taken: 963.1404552459717 sec\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "global_train_acc = []\n",
    "global_test_acc = []\n",
    "\n",
    "global_train_loss = []\n",
    "global_test_loss = []\n",
    "import time\n",
    "start_time = time.time()\n",
    "for e in range(n_epochs):\n",
    "    state = sess.run(initial_state)\n",
    "    iteration = 1\n",
    "    loss_=0.0\n",
    "    temp_train_loss = []\n",
    "    for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "        feed = {X: x, Y: y, initial_state: state, keep_prob: 0.5}\n",
    "\n",
    "        state, loss_,  _ = sess.run([final_state, loss, optimizer], feed_dict=feed)\n",
    "\n",
    "        if iteration%5==0:\n",
    "            print(\"Epoch: {}/{}\".format(e, n_epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Train loss: {:.5f}\".format(loss_))\n",
    "        temp_train_loss.append(loss_)\n",
    "        '''\n",
    "        if iteration%25==0:\n",
    "            val_acc = []\n",
    "            val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                feed = {X: x,\n",
    "                        Y: y,\n",
    "                        initial_state: val_state}\n",
    "                batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                val_acc.append(batch_acc)\n",
    "            print(\"Val acc: {:.5f}\".format(np.mean(val_acc)))\n",
    "        '''\n",
    "        '''\n",
    "        if iteration%25==0:\n",
    "            # train Acc calculation\n",
    "            train_acc = []\n",
    "            train_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batches(train_x, train_y, batch_size):\n",
    "                feed = {X: x,\n",
    "                        Y: y,\n",
    "                        initial_state: train_state}\n",
    "                batch_acc, train_state, corr = sess.run([accuracy, final_state, correct_pred], feed_dict=feed)\n",
    "                bad_indexes = [index for index, correctness in enumerate(corr) if correctness ==0 ]\n",
    "                train_acc.append(batch_acc)\n",
    "            print(\"Train acc: {:.5f}\".format(np.mean(train_acc)))\n",
    "            global_train_acc.append(np.mean(train_acc))\n",
    "            \n",
    "            # test acc calculation\n",
    "            test_acc = []\n",
    "            test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "                feed = {X: x,Y: y,initial_state: test_state}\n",
    "\n",
    "                batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                test_acc.append(batch_acc)\n",
    "            print(\"Train acc: {:.5f}\".format(np.mean(test_acc)))\n",
    "            global_test_acc.append(np.mean(test_acc))\n",
    "        '''    \n",
    "        iteration +=1    \n",
    "    global_train_loss.append(np.mean(temp_train_loss))\n",
    "    \n",
    "    temp_test_loss = []\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {X: x,Y: y,initial_state: test_state,  keep_prob: 1}\n",
    "\n",
    "        batch_acc, test_state, loss_ = sess.run([accuracy, final_state, loss], feed_dict=feed)\n",
    "        temp_test_loss.append(loss_)\n",
    "    \n",
    "    global_test_loss.append(np.mean(temp_test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Training Completed')\n",
    "print('Total Time Taken: '+str(time.time()-start_time)+' sec' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.49840\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "    feed = {X: x,Y: y,initial_state: test_state, keep_prob: 1}\n",
    "\n",
    "    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Train acc: {:.5f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#92\n",
      "Okay, first of all I got this movie as a Christmas present so it was FREE! FIRST - This movie was meant to be in stereoscopic 3D. It is for the most part, but whenever the main character is in her car the movie falls flat to 2D! What!!?!?! It's not that hard to film in a car!!! SECOND - The story isn't very good. There are a lot of things wrong with it.<br /><br />THIRD - Why are they showing all of the deaths in the beginning of the film! It made the movie suck whenever some was going to get killed!!! Watch it for a good laugh , but don't waste your time buying it. Just download it or something for cheap.\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#282\n",
      "When I heard Patrick Swayze was finally returning to his acting career with KING SOLOMON'S MINES I was very excited. I was expecting a great Indiana Jones type action adventure. What I got was a 4 hour long (with commercials) epic that was very slow. The second and third hour could have been dropped altogether and the story would not have suffered for it. The ending was good (no spoilers here)but I was still left wanting more. Well all a guy can do is prey that Swayze does \\RoadHouse 2\\\" so he can get back into the action genre that made him famous. Until than if your a fan of King Solomon's Mines than read the book or watch the 1985 version with Richard Chamberlain and Sharon Stone which is also not very good but its only and hour and forty minutes of your life gone instead of 4 hours.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#1184\n",
      "Brilliant execution in displaying once and for all, this time in the venue of politics, of how \\good intentions do actually pave the road to hell\\\". Excellent!\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#2331\n",
      "Very nice action with an interwoven story which actually doesn't suck. Interesting enough to merit watching instead of skipping past to get to the good parts. Having Jenna Jameson and Asia Carrere helps liven it up, too. Jenna in that sweater and those glasses is just astounding! Worth picking up just to see her!\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#2815\n",
      "I've just seen The Saint Strikes Back for the first time and found it quite good. This was George Sanders's first appearance as the Saint, where he replaces Louis Hayward.<br /><br />In this one, the Saint is sent to San Francisco to investigate a shooting at a night club. With the help of his acquaintance Inspector Fernack who has come down from New York, they help a daughter of a crime boss.<br /><br />Joining Sanders in the cast are Wendy Barrie and Jonathan Hale.<br /><br />Not a bad Saint movie. Worth seeing.<br /><br />Rating: 3 stars out of 5.\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#2848\n",
      "I have spent the last week watching John Cassavetes films - starting with 'a woman under the influence' and ending on 'opening night'. I am completely and utterly blown away, in particular by these two films. from the first minute to the last in 'opening night' i was completely and utterly absorbed. i've only experienced it on a few occasions, but the feeling that this film was perfect lasted from about two thirds in, right through till the credits came up. everything about this film, from the way it was shot, the incredible performance of Gena Rowlands, the credits, the opening, the music, the plot, the sense of depth, the pace, the tenderness, the originality, the characters, the deft little moments.... for me, is truly sublime. i couldn't agree more with the previous comment about taking it to a desert island because the sheer depth of this film is something to behold. if your unlucky enough to have a house fire, i guarantee that instead of making a last ditch attempt to rescue that stash of money under your bed, you'll be rescuing your copy of this film instead.\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#3233\n",
      "I saw this movie with my friend and we couldnt stop laughing! i mean there was nothing scary about this movie! It was funny all the lines Freddy said were hilarious! I think they shoudln't have even made a new nightmare and just gone to Freddy Vs. jason. Although some parts were gross (like the head blowing up). and any elm street film from 1- 5 sucked. this was the best besides Number 1. I wouldnt recomend this movie if you want a good horror. But if you have nothing else to do rent this and you'll laugh alot.I want to see the texas chainsaw massacre I think it would be scary. Freddy's Dead The Final Nightmare overall grade: B-\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#4833\n",
      "This is a great short. i think every voice is done by jason steele. (you can only just barely tell if you've heard his normal voice though, so don't worry about them sounding the same. they don't.) its about 15 minutes long.<br /><br />edward the spatula is fighting the war against spoons and he meets some weird people. in fact, everyone he knows seem pretty crazy. <br /><br />\\edward!\\\" \\\"general peterson, we have to get you to a medical unit!\\\" \\\"no, I'm not gonna make it edward.\\\" \\\"dont talk like that, I'm sure you'll be fine.\\\" \\\"im a goner edward, and you know it. before i go-\\\" \\\"yes?\\\" \\\"can i just have... one kiss?\\\" \\\"umm, no.\\\" \\\"come on, just one, small, peck on the lips?\\\" \\\"im walking away now sir.\\\"<br /><br />there's gonna be movie pretty soon. the date for that is in September, but its probably gonna get pushed back.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#7077\n",
      "Man, if anyone was expecting a great zombie movie after reading that title, then you are a retard and you deserve to be disappointed. As for myself, I was expecting a low-budgeted cheeseball zombie flick- and that's exactly what I got. I wasn't disappointed at all. I thought it was a cool little movie. The zombies were exactly as they should be, because all of the zombies had JUST been turned, so they are freshly-undead zombies. Obviously they did that because it would've been pretty costly if they had done full-on rotted zombie FX. I understood the whole thing, I have no idea how anyone could seriously nitpick this movie. It's called \\HOOD OF THE LIVING DEAD\\\" for the love of God! Would you watch \\\"Redneck Zombies\\\" and ANY Uwe Boll movie and actually EXPECT it to be great? Of course not! So why there are some morons on IMDb whining like school girls about this movie, I'll never understand. Oh and YES, there ARE worse movies out there, so stop saying that this was the worst you've ever seen, 'cause you know you're full of it! You ever watch \\\"ZOMBIEZ\\\"???? Or \\\"Feardotcom\\\"????? Or \\\"House Of The Dead\\\"???? THOSE are some of the worst I've ever seen. If you can't see that it's just a low- budgeted zombie movie obviously made by zombie movie fans, then something's wrong with you. I just had fun with it. Thumbs up from me and I'd also like to see a sequel.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#7713\n",
      "I'm a HUGE fan of the twin sisters. Although this was one of their \\not soo good\\\" movies. I'm not saying it's bad, I can't say it's bad, but this whole popular and not popular thingy isn't good. Although I give this movie a 4.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#7837\n",
      "I have screened this movie several times here at college, and every time I show it, the number of people watching with me grows exponentially... in addition to the virgins, anyone I've already shown it to NEEDS to see it again! It takes a little while to get into it, but by the end the whole room is screaming, shouting, yelling, rewinding scenes repeatedly, repeating dialogue, and just totally and completely engrossed in the moviegoing experience that is Pia Zadora in \\The Lonely Lady\\\"! Scene after scene after scene of the most ineptly filmed, poorly written, horribly acted TRASH is thrown at you in an all-out assault that ranks as the campiest thing I own (no small statement, friends). For me nothing compares 2 U, Pia... and I don't suppose I'm the only one who's ever felt this way!\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#8724\n",
      "There are so many words I want to use to describe this movie, but can't really do that can I? This movie is a movie to watch if you just want to sit, laugh, cry and then pee. I'm serious. Don't watch this movie if you're easily offended by profanity, sex, nudity, homosexuality...and everything else associated with nature. Being a woman, and that might not even be a factor, I can watch this movie over and over again. Trey Parker and Matt Stone are absolutely brilliant. Along with all their other debuts, I think Baseketball is the prize winner. I'm laughing now just thinking about some of the stupid things they do in the movie. Watch the movie!! That's all I'm going to say. It's sort of hard for me to leave this comment because I'm one of those people, like Ozzy Osbourne, who has a curse word in almost every line that blurts out of their mouth when they speak. So I'm keeping it professional. Best movie. Heck yeah!!\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#9364\n",
      "After viewing several episodes of this series, I have come to the conclusion that television producers are completely devoid of any form of originality. Here is an old science fiction standby, ingeniously wrapped in the form of a truly original concept - and still they can only -almost - make it work.<br /><br />The dialog is good! The male actors are reasonably proficient at their professions. Most of the characters are well drawn, with special kudos to the hero and his more than likeable side-kick. And most of the episode plots come across as palatable. So what could be wrong? How about the, the female characters and the cosmeticly perfect actresses who are chosen to portray them. <br /><br />The producers insist on portraying the female characters in this - almost good - series, in a manner that makes the end product appear to be a misplaced cheerleader. Why, I ask, why?<br /><br />The episodes all fall flat whenever the female guest star or recurring character comes on screen. These actresses are all totally unbelievable in their roles, and you don't actually have to see them to know they are incapable of their acting assignments. A blind person could tell. Just listen to them talk. They deliver their dialog with all the drama and effect of a 16 year old at the high school prom. Who would believe these women are Phd scientist, senators, corporate executives and medical doctors?<br /><br />In a nut shell, if the producers have their choice of a Stockard Channing or a Morgan Fairchild, guess who they'll choose - every time? And of course, the series suffers for it. Too bad!\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#10162\n",
      "The legendary Boris Karloff ended his illustrious career by making four cheapie fright flick clunkers in Mexico. This is the token moody period Gothic horror entry from the bunch. Karloff gives a typically spry and dignified performance as Matthias Morteval, an elderly eccentric patriarch who invites several of his petty, greedy and backbiting no-count relatives to his creepy rundown castle for the reading of a will. Pretty soon the hateful guests are getting bumped off by lethal life-sized toy people who populate the place. Onetime Mexican sex symbol Andres Garcia of \\Tintorera\\\" infamy portrays the dashing police officer hero and Julissa looks absolutely ravishing as the sole likable female character. The clunky, plodding (non)direction, trite by-the-numbers script, ugly, washed-out cinematography, ridiculous murder set pieces (a gross fat slob gets blasted right in the face by a miniature cannon!), overwrought string score, morbid gloom-doom atmosphere, largely lousy acting (Karloff notably excepted), cheesy mild gore, poor dubbing and rousing fiery conclusion all lend this enjoyably awful lemon a certain endearingly cruddy and hence oddly amusing ratty charm. A real campy hoot.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#10477\n",
      "I'm a fan of C&C, going back to their records, and liked this movie, but at one point in the mid-1980's on cable television in San Jose California, it was aired with an alternate plot line that destroyed the entire point of the movie. All references to marijuana were replaced with \\diamonds\\\". The bag that \\\"Red\\\" drops to Chong has diamonds in it instead of marijuana, but the conversation still remains the same (\\\"...it's worth ~$3000/lb\\\"). There is also a subplot in which clips of aliens on a ship were added observing C&C, and talking to each other about getting the diamonds. At the end, instead of \\\"space coke\\\", it's something else. I'm not sure who created this version, but it was horrible, and obvious that they were attempting to make it family/child friendly. It would have been better if that network had not aired it at all.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#12243\n",
      "This is simply the funniest movie I've seen in a long time. The bad acting, bad script, bad scenery, bad costumes, bad camera work and bad special effects are so stupid that you find yourself reeling with laughter.<br /><br />So it's not gonna win an Oscar but if you've got beer and friends round then you can't go wrong.\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#14445\n",
      "I would give this show a ten out of ten if it was not for the fart jokes. You people are so damn sensitive it is inane! So quick to point out the \\racism\\\" of the show and the jokes, yet are also so quick to say ridiculously sexist, pig-headed crap like \\\"well, duh, some of these other shows do these jokes so much better because at least they have hot women.\\\" So disgusting. Abortion jokes are great because, really, who takes abortion seriously anyways? At least I'm not a*bore*son. I hear that Reba McEntire and Sarah Silverman are teaming up to do a movie about sisters taking a road trip together. Talk about a movie of the year!\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#15076\n",
      "According to this board, I guess either you love it or hate it. Usually how it goes with all movies. There is no need to get testy with others though. All we are doing here is giving opinions. I rented this movie last night and I want to come and throw my opinion in the mix. I was surprised by how many people are thrashing it though. There's a difference between a movie fan and a horror movie fan. I'm a horror movie fan. Most plain ol' movie fans don't like horror movies. So many low budget cam corder looking movies are coming out these days. It's hard to keep up. And what makes it tough to stay into these movies is how bad they are. I wanted to come and write a review about \\Hood of the Living Dead\\\" because it's pretty damn good compared to the rest of the junk out there. It's nothing special but it's those horror film makers that try to be too serious that end up making a horrible horror film. I really liked this one. You telling me there is no effort in this one? And one more thing, I bet all of you have all of these huge DVD collections that you are so proud of, nothing but Major Motion Pictures right? Nothing wrong with that, but you have to know how to appreciate low-budget independent. I knew what I was getting when I watched this movie. I'm not going to be upset because I thought it was going to be some 100 million dollar movie. Some of you might need to stick with watching the Matrix over and over again and stop trying to compare everything to the Matrix. And if most of you are under 24, that explains everything. Good movie folks, check it out.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#18896\n",
      "The only reason I wanted to see this was because of Orlando Bloom. Simply put, the movie was spectacularly average. It's not bad, but it's really not very good. The editing is good; the film is well-paced. The direction is competent and assured. The story is plodding. The film is averagely acted by Ledger, Bloom, and the normally great Watts and Rush. The accents are impenetrable if you're from the US so just sit back and enjoy the scenery (or as I like to call it, Orlando Bloom). By the end of the film, I was neither bored nor moved. Some people have asked what happened to Ned Kelly at the end of the movie. I have to say, I so did not care by that point.<br /><br />Really, the only reason I can recommend this is that Orlando Bloom kind of, sort of shows some hints of range (although the oft-present \\I'm pretty and confused\\\" look is prominent), so fangirls may find it worth the matinee price. Other than that, just don't see it. It's neither good enough nor bad enough to be entertaining.\"\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#19741\n",
      "I love horses and admire hand drawn animation, so I expected nothing short of amazement from Dreamworks new animated picture Spirit: Stallion of the Cimarron. I guess you could say I was a little bit disappointed. You have wonderful animation and at first what seems like a perfect story. A story about absolutely nothing but a horse in nature. The animals don't sing cute songs or even talk -- a major plus. Sadly, the film has an uncalled for narration by Matt Damon; a sappy soundtrack by Bryan Adams; and enough action scenes to compare it to a Jerry Bruckheimer production. If the film makers would have just stayed with simplicity, we'd have a masterpiece here. This is not a great film, but it is good entertainment for small children. I would recommend this film to families because it has its heart in the right place and its the only thing out there right now that isn't offensive to small children. Not bad, but could have been much better. Very pretty visuals though.\n",
      "-->Label=[1]\n",
      "\n",
      "\n",
      "#19895\n",
      "For anyone who may not know what a one-actor movie was like, this is the best example. This plot is ridiculous, and really makes no sense. It's full of cliched situations, hackneyed lines, melodrama, comedy... you name it!<br /><br />But Amitabh Bachchan can make anything convincing, and this movie is by no means an exception. Everyone turns in a decent performance - Shashi Kapoor, Waheeda Rehman, Ranjit, Om Prakash, Smita Patil... But it is the Megastar who overshadows everyone with his towering presence. Without him, this movie would have been a non-starter... The story is about separation / mistaken identities / misunderstandings / love / hate / loyalty / good vs evil - everything, really! Amitabh's is a brilliant performance on all counts, in an otherwise silly film! And did I mention that it is ridiculously funny?\n",
      "-->Label=[1]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "wrong_temp = [92, 282, 1184, 2331, 2815, 2848, 3233, 4833, 7077, 7713, 7837, 8724, 9364, 10162, 10477, 12243, 14445, 15076, 18896, 19741, 19895]\n",
    "for i in wrong_temp[:]:\n",
    "    print('#'+str(i))\n",
    "    print(reviews[i])\n",
    "    print('-->Label='+str(labels[i]))\n",
    "    print()\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ming The Merciless does a little Bardwork and a movie most foul!',\n",
       " 22258,\n",
       " [0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_l = 0\n",
    "min_l = 10000\n",
    "i = 0\n",
    "m = -1\n",
    "for review in reviews_pp_ints:\n",
    "\n",
    "    if len(review) > max_l:\n",
    "        max_l = len(review)\n",
    "    if len(review) < min_l:\n",
    "        min_l = len(review)\n",
    "        m = i\n",
    "    i += 1\n",
    "reviews[m], m, labels[m]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count['terrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2715"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count['horror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [92, 282, 1184, 2331, 2815, 2848, 3233, 4833, 7077, 7713, 7837, 8724, 9364, 10162, 10477, 12243, 14445, 15076, 18896, 19741, 19895]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores of Bad Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor index in a:\\n    state = sess.run(initial_state)\\n\\n    feed = {X: features[index].reshape(1,None) , Y: labels[index].reshape(1,None), initial_state: state}\\n\\n        outputs_ = sess.run([outputs], feed_dict=feed)\\n\\n    print(\"Index:() \".format(index))\\n    print(\"Prediction:{} \".format(outputs_))\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for index in a:\n",
    "    state = sess.run(initial_state)\n",
    "\n",
    "    feed = {X: features[index].reshape(1,None) , Y: labels[index].reshape(1,None), initial_state: state}\n",
    "\n",
    "        outputs_ = sess.run([outputs], feed_dict=feed)\n",
    "\n",
    "    print(\"Index:() \".format(index))\n",
    "    print(\"Prediction:{} \".format(outputs_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Test Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[1732.097, 317.04868, 42.455299, 4.1286335, 0.48109666, 0.25812858, 0.25018141, 0.25004876, 0.25004956, 0.25005072, 0.25005177, 0.25005269, 0.25005347, 0.25005412, 0.25005472, 0.25005531, 0.25005573, 0.25005618, 0.25005656, 0.25005698, 0.25005731, 0.25005752, 0.25005788, 0.25005865, 0.25005871, 0.25005895, 0.25005931, 0.25005931, 0.2500602, 0.25005993, 0.25006109, 0.25006181, 0.25006664, 0.25010651, 0.25009227, 0.25008768, 0.25008756, 0.25008753, 0.25008968, 0.25008476, 0.25008541, 0.25008583, 0.25008672, 0.25008559, 0.25008792, 0.25008759, 0.25008926, 0.2500917, 0.25010484, 0.25007907, 0.25007856, 0.25007993, 0.25007945, 0.25010499, 0.25018105, 0.25013834, 0.25014001, 0.25011641, 0.2501058, 0.25010327]\n"
     ]
    }
   ],
   "source": [
    "print(len(global_train_loss))\n",
    "print(global_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for i in range( len(global_train_acc) ):\n",
    "    global_train_acc[i] /=100\n",
    "    global_test_acc[i] /=100\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3936.8328, 870.70801, 143.63805, 17.285648, 1.6351658, 0.32228947, 0.25223947, 0.25010854, 0.25008121, 0.25008264, 0.25008392, 0.25008497, 0.25008592, 0.25008669, 0.25008741, 0.25008801, 0.2500886, 0.25008905, 0.2500895, 0.25008991, 0.25009021, 0.25009051, 0.25009093, 0.25009137, 0.2500917, 0.25009209, 0.25009218, 0.2500926, 0.25009292, 0.25009322, 0.25009361, 0.25009465, 0.2500965, 0.25014892, 0.25012994, 0.2501238, 0.25011995, 0.25012091, 0.2501213, 0.25011975, 0.25011784, 0.25011891, 0.25011975, 0.25011966, 0.25011978, 0.25012127, 0.25012192, 0.25012341, 0.25012764, 0.2501173, 0.2501114, 0.25011206, 0.25011224, 0.25011575, 0.25018805, 0.2501896, 0.25018039, 0.25015876, 0.25014466, 0.25013864]\n",
      "[1732.097, 317.04868, 42.455299, 4.1286335, 0.48109666, 0.25812858, 0.25018141, 0.25004876, 0.25004956, 0.25005072, 0.25005177, 0.25005269, 0.25005347, 0.25005412, 0.25005472, 0.25005531, 0.25005573, 0.25005618, 0.25005656, 0.25005698, 0.25005731, 0.25005752, 0.25005788, 0.25005865, 0.25005871, 0.25005895, 0.25005931, 0.25005931, 0.2500602, 0.25005993, 0.25006109, 0.25006181, 0.25006664, 0.25010651, 0.25009227, 0.25008768, 0.25008756, 0.25008753, 0.25008968, 0.25008476, 0.25008541, 0.25008583, 0.25008672, 0.25008559, 0.25008792, 0.25008759, 0.25008926, 0.2500917, 0.25010484, 0.25007907, 0.25007856, 0.25007993, 0.25007945, 0.25010499, 0.25018105, 0.25013834, 0.25014001, 0.25011641, 0.2501058, 0.25010327]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFKjDABJapQBIEmNMgAkqVH/H5rUWVrCIFYRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e75a29588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Testing Loss: 0.250049\n"
     ]
    }
   ],
   "source": [
    "print(global_train_loss)\n",
    "print(global_test_loss)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.ion()\n",
    "x = range(60) \n",
    "plt.axis([0,70,0,0.3])\n",
    "plt.plot(x,global_train_loss,'go',x,global_test_loss,'ro')\n",
    "plt.show()\n",
    "\n",
    "print('Minimum Testing Loss: '+str(np.min(global_test_loss)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, first of all I got this movie as a Christmas present so it was FREE! FIRST - This movie was meant to be in stereoscopic 3D. It is for the most part, but whenever the main character is in her car the movie falls flat to 2D! What!!?!?! It's not that hard to film in a car!!! SECOND - The story isn't very good. There are a lot of things wrong with it.<br /><br />THIRD - Why are they showing all of the deaths in the beginning of the film! It made the movie suck whenever some was going to get killed!!! Watch it for a good laugh , but don't waste your time buying it. Just download it or something for cheap.\n"
     ]
    }
   ],
   "source": [
    "print(reviews[92])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
