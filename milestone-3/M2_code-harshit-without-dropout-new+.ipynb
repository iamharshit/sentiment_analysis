{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MILESTONE 2\n",
    "\n",
    "IMDB dataset + Siraj's Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(sentences):\n",
    "    words = []\n",
    "    for each in sentences:\n",
    "        each = each.lower()\n",
    "        words_this_sentence = each.split(' ')\n",
    "        words.extend(words_this_sentence)\n",
    "    vocab = set(words)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = []\n",
    "score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('beam_cable_google_tagged_data.json') as data_file:\n",
    "    data1 = json.load(data_file)\n",
    "with open('droom_google_tagged_data.json') as data_file:\n",
    "    data2 = json.load(data_file)\n",
    "for d in data1['data']:\n",
    "    conversation = d['conversation']\n",
    "    c_id = d['conversation_id']\n",
    "    for c in conversation:\n",
    "        sentiment = float(c['sentiment'])\n",
    "        sentiment = -1 if (sentiment < 0) else 1\n",
    "        dialogue = c['text']\n",
    "        sentence.append(dialogue)\n",
    "        score.append(sentiment)\n",
    "\n",
    "for d in data2['data']:\n",
    "    conversation = d['conversation']\n",
    "    c_id = d['conversation_id']\n",
    "    for c in conversation:\n",
    "        sentiment = float(c['sentiment'])\n",
    "        sentiment = -1 if (sentiment < 0) else 1\n",
    "        dialogue = c['text']\n",
    "        sentence.append(dialogue)\n",
    "        score.append(sentiment)\n",
    "\n",
    "# sentence.extend(dialogues)\n",
    "# score.extend(sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_chat = list(get_vocab(sentence))\n",
    "# print(len(vocab_chat), vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "filename = 'Embedding_models/glove.6B.300d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.825595464771767\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for each in sentence:\n",
    "    total = total + len(each.split(' '))\n",
    "avg = total/len(sentence)\n",
    "print (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_document_length = 1\n",
    "# vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "# #fit the vocab from glove\n",
    "# pretrain = vocab_processor.fit(vocab)\n",
    "# #transform inputs\n",
    "# all_w = []\n",
    "# total = 0\n",
    "# for each in vocab_chat:\n",
    "#     total = total+1\n",
    "#     input_x = np.array(list(vocab_processor.transform([each])))\n",
    "# #     print (input_x[0][0])\n",
    "#     if input_x[0][0] == 0:\n",
    "#         all_w.append(each)\n",
    "# print (total)\n",
    "# print (len(all_w))\n",
    "# with open('words_not_in_glove.txt','w') as out:\n",
    "#     out.write(str(all_w))\n",
    "            \n",
    "        \n",
    "# # print(np.shape(input_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "lstm_size_list = [64,128,256,501]\n",
    "lstm_layers_list = [1]\n",
    "batch_size_list = [100,250,500]\n",
    "n_epochs_list = [1,10,20,30]\n",
    "'''\n",
    "lstm_size_list = [128,256]\n",
    "lstm_layers_list = [1]\n",
    "batch_size_list = [100,250]\n",
    "n_epochs_list = [1,10]\n",
    "\n",
    "all_hyperpara_list = []\n",
    "for a in lstm_size_list:\n",
    "    for b in lstm_layers_list:\n",
    "        for c in batch_size_list:\n",
    "            for d in n_epochs_list:\n",
    "                all_hyperpara_list.append((a,b,c,d))\n",
    "\n",
    "i=0\n",
    "all_hyperpara_list = all_hyperpara_list[i:i+4]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "iter_ = i\n",
    "for hyperameter_tuple in all_hyperpara_list: \n",
    "    #Reset Graph\n",
    "    from tensorflow.python.framework import ops\n",
    "    ops.reset_default_graph()\n",
    "    \n",
    "    from tensorflow.contrib import learn\n",
    "    #init vocab processor\n",
    "    max_document_length = 20\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    #fit the vocab from glove\n",
    "    pretrain = vocab_processor.fit(vocab)\n",
    "    #transform inputs\n",
    "    input_x = np.array(list(vocab_processor.transform(sentence)))\n",
    "    print(np.shape(input_x))\n",
    "    \n",
    "    ############# NEW GLOVE ################\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                    trainable=False, name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    #######################################\n",
    "    \n",
    "    #Splitting\n",
    "    split_frac = 0.6\n",
    "    split_idx = int(len(sentence)*split_frac)\n",
    "\n",
    "    dialogues_train, dialogues_val = input_x[:split_idx], input_x[split_idx:]\n",
    "    sentiments_train, sentiments_val = score[:split_idx], score[split_idx:]\n",
    "\n",
    "    test_idx = int(len(dialogues_val)*0.5)\n",
    "    dialogues_val, dialogues_test = dialogues_val[:test_idx], dialogues_val[test_idx:]\n",
    "    sentiments_val, sentiments_test = sentiments_val[:test_idx], sentiments_val[test_idx:]\n",
    "    \n",
    "    train_x = []\n",
    "    val_x = []\n",
    "    train_y = []\n",
    "    val_y = []\n",
    "\n",
    "    train_x.extend(dialogues_train)\n",
    "    train_y.extend(sentiments_train)\n",
    "    \n",
    "    #Shuffling Dataset\n",
    "    from random import shuffle\n",
    "    train = list(zip(train_x, train_y))\n",
    "    shuffle(train)\n",
    "    train_x = [ item[0] for item in train]\n",
    "    train_y = [ [item[1]] for item in train]\n",
    "    sentiments_val = [ [item] for item in sentiments_val]\n",
    "    sentiments_test = [ [item] for item in sentiments_test]\n",
    "    \n",
    "    global_result = []\n",
    "\n",
    "    import csv\n",
    "\n",
    "    def file_writer(file_name,row_):\n",
    "        with open(file_name, 'a') as f:\n",
    "            spamwriter = csv.writer(f, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            spamwriter.writerow(row_ )\n",
    "\n",
    "    import time\n",
    "    t = time.time()\n",
    "\n",
    "    x_ = []\n",
    "    \n",
    "    lstm_size = hyperameter_tuple[0]\n",
    "    lstm_layers = hyperameter_tuple[1]\n",
    "    batch_size = hyperameter_tuple[2]\n",
    "    n_epochs = hyperameter_tuple[3]\n",
    "    \n",
    "    X = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "    Y = tf.placeholder(tf.float32, [None, 1], name = 'labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    \n",
    "    tf.set_random_seed(5)\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(W, X)\n",
    "    \n",
    "    # print (embed)\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm]*lstm_layers)\n",
    "\n",
    "    #getting an initial state of zeros\\n\",\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n",
    "\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1],1, activation_fn=tf.tanh)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(Y - predictions))\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    #Accuracy:\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.float32), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    '''\n",
    "    #tp, fp, tn, fn:\n",
    "    temp_1 = tf.Variable(tf.cast([1.0]*batch_size,tf.float32))\n",
    "    temp_negative_1 = tf.Variable(tf.cast([-1.0]*batch_size, tf.float32))\n",
    "    tp = tf.reduce_sum( tf.cast( tf.equal(tf.cast(correct_pred, tf.float32), temp_1),tf.float32 ) )\n",
    "    tn = tf.reduce_sum( tf.cast( tf.equal(tf.cast(correct_pred, tf.float32), temp_negative_1 ),tf.float32 ) )\n",
    "    \n",
    "    temp1 =  tf.cast(tf.equal(tf.cast(tf.round(predictions), tf.float32), temp_1),tf.float32)\n",
    "    temp2 = tf.cast(tf.equal(tf.cast(tf.round(predictions), tf.float32), temp_negative_1),tf.float32)\n",
    "    fp = tf.reduce_sum( tf.cast(tf.equal(temp1,temp2),tf.float32 ))\n",
    "    \n",
    "    temp11 =  tf.cast(tf.equal(tf.cast(tf.round(predictions), tf.float32), temp_negative_1),tf.float32)\n",
    "    temp12 = tf.cast(tf.equal(tf.cast(tf.round(predictions), tf.float32), temp_1),tf.float32)\n",
    "    fn = tf.reduce_sum( tf.cast(tf.equal(temp11,temp12),tf.float32 ))\n",
    "    '''\n",
    "    #recall = tf.metrics.recall(Y,predictions) \n",
    "    #precision = tf.metrics.precision(Y,predictions) \n",
    "    \n",
    "    def get_batches(x, y, batch_size=100):\n",
    "        n_batches = len(x)//batch_size\n",
    "        x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "        for ii in range(0, len(x), batch_size):\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "            \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        iteration = 1\n",
    "        \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {X: x, Y: y, initial_state: state}\n",
    "            state, loss_,  _ = sess.run([final_state, loss, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%50==0:\n",
    "                #Train Loss:\n",
    "                print(\"Epoch: {}/{}\".format(e, n_epochs),\"Iteration: {}\".format(iteration),\"Train loss: {:.5f}\".format(loss_))\n",
    "                train_loss.append(loss_)\n",
    "                x_.append(ii)\n",
    "                \n",
    "                #Validation loss:\n",
    "                temp_val_loss = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(dialogues_val, sentiments_val, batch_size):\n",
    "                    feed = {X: x,Y: y,initial_state: val_state}\n",
    "                    loss_, val_state = sess.run([loss, final_state], feed_dict=feed)\n",
    "                    temp_val_loss.append(loss_)\n",
    "                val_loss.append(np.mean(temp_val_loss))\n",
    "                     \n",
    "            iteration+=1\n",
    "            \n",
    "        #Train Acc:\n",
    "        train_acc = []\n",
    "        train_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {X: x,Y: y,initial_state: train_state}\n",
    "\n",
    "            batch_acc, train_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "            train_acc.append(batch_acc)\n",
    "        print(\"Train accuracy: {:.5f}\".format(np.mean(train_acc)))\n",
    "        file_writer('hyper_'+str(iter_)+'.csv',['Epoch= '+str(e),'Train Acc= '+str(np.mean(train_acc)) ])\n",
    "\n",
    "    print('Training Completed')\n",
    "    \n",
    "    #Time:\n",
    "    time_taken = time.time()-start_time \n",
    "    \n",
    "    #Graph:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline \n",
    "    plt.ion()\n",
    "    plt.axis([0,max(x_)+5,0,1])\n",
    "    plt.plot(x_,train_loss,'go',x_,val_loss,'bo')\n",
    "    plt.savefig(\"figure\"+str(iter_)+\".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    #Write Time_Taken, Min_Val_loss\n",
    "    lstm_size = hyperameter_tuple[0]\n",
    "    lstm_layers = hyperameter_tuple[1]\n",
    "    batch_size = hyperameter_tuple[2]\n",
    "    n_epochs = hyperameter_tuple[3]\n",
    "    file_writer('hyper_'+str(iter_)+'.csv',['lstm_size= '+str(lstm_size), 'lstm_layers= '+str(lstm_layers),'batch_size= '+str(batch_size),'n_epochs= '+str(n_epochs)])\n",
    "    file_writer('hyper_'+str(iter_)+'.csv',['time_taken= '+str(time_taken),'min val loss= '+str(np.min(val_loss)) ])\n",
    "    \n",
    "    #Hyperparameter S.No.\n",
    "    iter_+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Test Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
