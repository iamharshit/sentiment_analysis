{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MILESTONE 2\n",
    "\n",
    "IMDB dataset + Siraj's Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(sentences):\n",
    "    words = []\n",
    "    for each in sentences:\n",
    "        each = each.lower()\n",
    "        words_this_sentence = each.split(' ')\n",
    "        words.extend(words_this_sentence)\n",
    "    vocab = set(words)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = []\n",
    "score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('beam_cable_google_tagged_data.json') as data_file:\n",
    "    data1 = json.load(data_file)\n",
    "with open('droom_google_tagged_data.json') as data_file:\n",
    "    data2 = json.load(data_file)\n",
    "for d in data1['data']:\n",
    "    conversation = d['conversation']\n",
    "    c_id = d['conversation_id']\n",
    "    for c in conversation:\n",
    "        sentiment = float(c['sentiment'])\n",
    "        sentiment = -1 if (sentiment < 0) else 1\n",
    "        dialogue = c['text']\n",
    "        sentence.append(dialogue)\n",
    "        score.append(sentiment)\n",
    "\n",
    "for d in data2['data']:\n",
    "    conversation = d['conversation']\n",
    "    c_id = d['conversation_id']\n",
    "    for c in conversation:\n",
    "        sentiment = float(c['sentiment'])\n",
    "        sentiment = -1 if (sentiment < 0) else 1\n",
    "        dialogue = c['text']\n",
    "        sentence.append(dialogue)\n",
    "        score.append(sentiment)\n",
    "\n",
    "# sentence.extend(dialogues)\n",
    "# score.extend(sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_chat = list(get_vocab(sentence))\n",
    "# print(len(vocab_chat), vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Embedding_models/glove.6B.300d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for each in sentence:\n",
    "    total = total + len(each.split(' '))\n",
    "avg = total/len(sentence)\n",
    "print (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "#init vocab processor\n",
    "max_document_length = 20\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "#transform inputs\n",
    "input_x = np.array(list(vocab_processor.transform(sentence)))\n",
    "print(np.shape(input_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_document_length = 1\n",
    "# vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "# #fit the vocab from glove\n",
    "# pretrain = vocab_processor.fit(vocab)\n",
    "# #transform inputs\n",
    "# all_w = []\n",
    "# total = 0\n",
    "# for each in vocab_chat:\n",
    "#     total = total+1\n",
    "#     input_x = np.array(list(vocab_processor.transform([each])))\n",
    "# #     print (input_x[0][0])\n",
    "#     if input_x[0][0] == 0:\n",
    "#         all_w.append(each)\n",
    "# print (total)\n",
    "# print (len(all_w))\n",
    "# with open('words_not_in_glove.txt','w') as out:\n",
    "#     out.write(str(all_w))\n",
    "            \n",
    "        \n",
    "# # print(np.shape(input_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e484471fa4e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m############# NEW GLOVE ################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n\u001b[0m\u001b[1;32m      4\u001b[0m                 trainable=False, name=\"W\")\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "############# NEW GLOVE ################\n",
    "\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_frac = 0.6\n",
    "split_idx = int(len(sentence)*split_frac)\n",
    "\n",
    "dialogues_train, dialogues_val = input_x[:split_idx], input_x[split_idx:]\n",
    "sentiments_train, sentiments_val = score[:split_idx], score[split_idx:]\n",
    "\n",
    "test_idx = int(len(dialogues_val)*0.5)\n",
    "dialogues_val, dialogues_test = dialogues_val[:test_idx], dialogues_val[test_idx:]\n",
    "sentiments_val, sentiments_test = sentiments_val[:test_idx], sentiments_val[test_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = []\n",
    "val_x = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "\n",
    "# train_x.extend(reviews_train)\n",
    "train_x.extend(dialogues_train)\n",
    "\n",
    "# train_y.extend(labels_train)\n",
    "train_y.extend(sentiments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "train = list(zip(train_x, train_y))\n",
    "shuffle(train)\n",
    "\n",
    "train_x = [ item[0] for item in train]\n",
    "train_y = [ [item[1]] for item in train]\n",
    "\n",
    "sentiments_val = [ [item] for item in sentiments_val]\n",
    "sentiments_test = [ [item] for item in sentiments_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "#hidden_nodes = 10\n",
    "n_epochs = 1\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name = 'labels')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "tf.set_random_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-05d77d3003f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#getting an initial state of zeros\\n\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36mzero_state\u001b[0;34m(self, batch_size, dtype)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"ZeroState\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_is_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;31m# We know here that state_size of each cell is not a tuple and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"ZeroState\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_is_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;31m# We know here that state_size of each cell is not a tuple and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mzero_state\u001b[0;34m(self, batch_size, dtype)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"ZeroState\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mstate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_zero_state_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m_zero_state_tensors\u001b[0;34m(state_size, batch_size, dtype)\u001b[0m\n\u001b[1;32m     56\u001b[0m             array_ops.stack(_state_size_with_prefix(\n\u001b[1;32m     57\u001b[0m                 s, prefix=[batch_size])),\n\u001b[0;32m---> 58\u001b[0;31m             dtype=dtype) for s in state_size_flat\n\u001b[0m\u001b[1;32m     59\u001b[0m     ]\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m             array_ops.stack(_state_size_with_prefix(\n\u001b[1;32m     57\u001b[0m                 s, prefix=[batch_size])),\n\u001b[0;32m---> 58\u001b[0;31m             dtype=dtype) for s in state_size_flat\n\u001b[0m\u001b[1;32m     59\u001b[0m     ]\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    815\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m   \u001b[0mvalue_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mexpanded_num_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    637\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    111\u001b[0m                                          as_ref=False):\n\u001b[1;32m    112\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 102\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    103\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    358\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "#embed = tf.nn.embedding_lookup(W, X)\n",
    "# print (embed)\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm]*lstm_layers)\n",
    "\n",
    "#getting an initial state of zeros\\n\",\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n",
    "\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1],1, activation_fn=tf.tanh)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(Y - predictions))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accuracy:\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.float32), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1 Iteration: 50 Train loss: 2.75200\n",
      "Epoch: 0/1 Iteration: 100 Train loss: 2.56800\n",
      "Epoch: 0/1 Iteration: 150 Train loss: 2.80800\n",
      "Epoch: 0/1 Iteration: 200 Train loss: 2.76000\n",
      "Val acc: 0.29363\n",
      "Train accuracy: 0.30720\n",
      "Epoch: 0/1 Iteration: 250 Train loss: 2.89600\n",
      "Epoch: 0/1 Iteration: 300 Train loss: 2.80000\n",
      "Epoch: 0/1 Iteration: 350 Train loss: 2.59200\n",
      "Epoch: 0/1 Iteration: 400 Train loss: 2.64800\n",
      "Val acc: 0.29363\n",
      "Train accuracy: 0.30720\n",
      "Epoch: 0/1 Iteration: 450 Train loss: 2.75200\n",
      "Epoch: 0/1 Iteration: 500 Train loss: 2.72000\n",
      "Epoch: 0/1 Iteration: 550 Train loss: 2.84800\n",
      "Epoch: 0/1 Iteration: 600 Train loss: 2.91200\n",
      "Val acc: 0.29363\n",
      "Train accuracy: 0.30720\n",
      "Epoch: 0/1 Iteration: 650 Train loss: 2.72800\n",
      "Epoch: 0/1 Iteration: 700 Train loss: 2.65600\n",
      "Epoch: 0/1 Iteration: 750 Train loss: 2.79200\n",
      "Epoch: 0/1 Iteration: 800 Train loss: 2.74400\n",
      "Val acc: 0.29363\n",
      "Train accuracy: 0.30720\n",
      "Epoch: 0/1 Iteration: 850 Train loss: 2.65600\n",
      "Training Completed\n",
      "Total Time Taken: 151.97952008247375 sec\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "global_train_acc = []\n",
    "global_test_acc = []\n",
    "\n",
    "global_train_loss = []\n",
    "global_test_loss = []\n",
    "\n",
    "global_val_chat_loss = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "min_loss = 1.0\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: embedding})\n",
    "for e in range(n_epochs):\n",
    "    state = sess.run(initial_state)\n",
    "    iteration = 1\n",
    "    temp_train_loss = []\n",
    "    for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "        feed = {X: x, Y: y, initial_state: state}\n",
    "\n",
    "        state, loss_,  _ = sess.run([final_state, loss, optimizer], feed_dict=feed)\n",
    "\n",
    "        if iteration%50==0:\n",
    "            print(\"Epoch: {}/{}\".format(e, n_epochs),\n",
    "                  \"Iteration: {}\".format(iteration),\n",
    "                  \"Train loss: {:.5f}\".format(loss_))\n",
    "        temp_train_loss.append(loss_)\n",
    "        \n",
    "        if iteration%200==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(dialogues_val, sentiments_val, batch_size):\n",
    "                    feed = {X: x,\n",
    "                            Y: y,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.5f}\".format(np.mean(val_acc)))\n",
    "                \n",
    "                train_acc = []\n",
    "\n",
    "                train_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "                    feed = {X: x,Y: y,initial_state: train_state}\n",
    "\n",
    "                    batch_acc, train_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    train_acc.append(batch_acc)\n",
    "\n",
    "                print(\"Train accuracy: {:.5f}\".format(np.mean(train_acc)))\n",
    "        \n",
    "        '''\n",
    "        if loss_<min_loss:\n",
    "            min_loss = loss_\n",
    "            save_path = saver.save(sess, \"/model.ckpt\")\n",
    "        '''\n",
    "        iteration+=1\n",
    "    global_train_loss.append(np.mean(temp_train_loss))\n",
    "    \n",
    "    \n",
    "    ##Validation loss Chat\n",
    "    val_loss = []\n",
    "    val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for x, y in get_batches(dialogues_val, sentiments_val, batch_size):\n",
    "        feed = {X: x,\n",
    "                Y: y,\n",
    "                initial_state: val_state}\n",
    "        loss_, val_state = sess.run([loss, final_state], feed_dict=feed)\n",
    "        val_loss.append(loss_)\n",
    "    global_val_chat_loss.append(np.mean(val_loss))\n",
    "    \n",
    "#sess.close()\n",
    "    \n",
    "print('Training Completed')\n",
    "print('Total Time Taken: '+str(time.time()-start_time)+' sec' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc for chat : 0.34712\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"/model.ckpt\")\n",
    "'''\n",
    "\n",
    "\n",
    "test_acc = []\n",
    "test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "for ii, (x, y) in enumerate(get_batches(dialogues_test, sentiments_test, batch_size), 1):\n",
    "    feed = {X: x,Y: y,initial_state: test_state, keep_prob: 1}\n",
    "\n",
    "    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "    test_acc.append(batch_acc)\n",
    "print(\"Test acc for chat : {:.5f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Test Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7691255]\n",
      "[2.8254867]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16a8051668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(global_train_loss)\n",
    "\n",
    "print(global_val_chat_loss)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.ion()\n",
    "x = range(n_epochs) \n",
    "plt.axis([0,len(x)+5,0,1])\n",
    "plt.plot(x,global_train_loss,'go',x,global_val_chat_loss,'bo')\n",
    "plt.show()\n",
    "\n",
    "#print('Minimum Testing Loss: '+str(np.min(global_test_loss)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
