{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tensorflow.contrib import learn\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_(s):\n",
    "    pattern = r'''\\d+|[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]|[A-Z][A-Z]+|http[s]?://[\\w\\./]+|[\\w]+@[\\w]+\\.[\\w]+|[a-z][a-z]+|[A-Za-z]\\.[\\w][\\w\\.]+|[\\w]+|[-'a-z]+|[\\S]+'''\n",
    "    \n",
    "    l = re.findall(pattern, s)\n",
    "    return l\n",
    "\n",
    "#Variables\n",
    "sentence_train = []\n",
    "score_train = []\n",
    "sentence_test = []\n",
    "score_test = []\n",
    "sentence_valid = []\n",
    "score_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Chat Dataset (from csv to list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'train.csv'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f, delimiter='$')\n",
    "    \n",
    "    for row in reader:\n",
    "        score_train.append(int(row[0]))\n",
    "        sentence_train.append(row[1])\n",
    "\n",
    "filename = 'valid.csv'\n",
    "with open(filename, 'r') as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f, delimiter='$')\n",
    "    \n",
    "    for row in reader:\n",
    "        score_valid.append(int(row[0]))\n",
    "        sentence_valid.append(row[1])\n",
    "\n",
    "filename = 'test.csv'\n",
    "with open(filename, 'r') as f:\n",
    "    next(f)\n",
    "    reader = csv.reader(f, delimiter='$')\n",
    "    \n",
    "    for row in reader:\n",
    "        score_test.append(int(row[0]))\n",
    "        sentence_test.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Glove (common crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'glove.840B.300d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        if (len(row[1:]) == 300):\n",
    "            vocab.append(row[0])\n",
    "            embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "vocab,embd = loadGloVe(filename)\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = len(embd[0])\n",
    "embedding = np.asarray(embd)\n",
    "vocab = set(vocab)\n",
    "word_to_int = {word:i for i,word in enumerate(vocab,1)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting FULL Sentence to its Integer representation (not embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################## to make features of the train dataset #####################\n",
    "sentence2ints = []\n",
    "for each in sentence_train:\n",
    "    each = tokenize_(each)\n",
    "    this_sentence_int = []\n",
    "    for word in each:\n",
    "        if word in vocab:\n",
    "            this_sentence_int.append(word_to_int[word])\n",
    "    sentence2ints.append(this_sentence_int)\n",
    "\n",
    "max_seq_len = 20\n",
    "features_train = np.zeros((len(sentence2ints), max_seq_len), dtype=int)\n",
    "for i, row in enumerate(sentence2ints):\n",
    "    features_train[i, :len(row)] = np.array(row[:max_seq_len] )\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "################## to make features of the test dataset #####################\n",
    "sentence2ints = []\n",
    "for each in sentence_test:\n",
    "    each = tokenize_(each)\n",
    "    this_sentence_int = []\n",
    "    for word in each:\n",
    "        if word in vocab:\n",
    "            this_sentence_int.append(word_to_int[word])\n",
    "    sentence2ints.append(this_sentence_int)\n",
    "\n",
    "max_seq_len = 20\n",
    "features_test = np.zeros((len(sentence2ints), max_seq_len), dtype=int)\n",
    "for i, row in enumerate(sentence2ints):\n",
    "    features_test[i, :len(row)] = np.array(row[:max_seq_len] )\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "################## to make features of the valid dataset #####################\n",
    "sentence2ints = []\n",
    "for each in sentence_valid:\n",
    "    each = tokenize_(each)\n",
    "    this_sentence_int = []\n",
    "    for word in each:\n",
    "        if word in vocab:\n",
    "            this_sentence_int.append(word_to_int[word])\n",
    "    sentence2ints.append(this_sentence_int)\n",
    "\n",
    "max_seq_len = 20\n",
    "features_val = np.zeros((len(sentence2ints), max_seq_len), dtype=int)\n",
    "for i, row in enumerate(sentence2ints):\n",
    "    features_val[i, :len(row)] = np.array(row[:max_seq_len] )\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting FULL Sentence to its Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Build Look up table\n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),\n",
    "                trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None], name = 'inputs')\n",
    "embed = tf.nn.embedding_lookup(W, X)\n",
    "\n",
    "#Start Session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    _,em_train = sess.run([embedding_init, embed], feed_dict={embedding_placeholder:embedding, X:features_train})\n",
    "    _,em_test = sess.run([embedding_init, embed], feed_dict={embedding_placeholder:embedding, X:features_test})\n",
    "    _,em_val = sess.run([embedding_init, embed], feed_dict={embedding_placeholder:embedding, X:features_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Chat Dataset's as its Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_x = open('train_x.pkl', 'wb')\n",
    "pickle.dump(em_train, train_x)\n",
    "train_x.close()\n",
    "\n",
    "\n",
    "val_x = open('val_x.pkl', 'wb')\n",
    "pickle.dump(em_val, val_x)\n",
    "val_x.close()\n",
    "\n",
    "\n",
    "test_x = open('test_x.pkl', 'wb')\n",
    "pickle.dump(em_test, test_x)\n",
    "test_x.close()\n",
    "\n",
    "train_y = open('train_y.pkl', 'wb')\n",
    "pickle.dump(score_train, train_y)\n",
    "train_y.close()\n",
    "\n",
    "\n",
    "test_y = open('test_y.pkl', 'wb')\n",
    "pickle.dump(score_test, test_y)\n",
    "test_y.close()\n",
    "\n",
    "\n",
    "val_y = open('val_y.pkl', 'wb')\n",
    "pickle.dump(score_valid, val_y)\n",
    "val_y.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
