{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "from pprint import pprint\n",
    "from tensorflow.contrib import learn\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Chat Dataset From Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl_file = open('train_x.pkl', 'rb')\n",
    "train_x = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('test_x.pkl', 'rb')\n",
    "test_x = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('val_x.pkl', 'rb')\n",
    "val_x = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('train_y.pkl', 'rb')\n",
    "train_y = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('test_y.pkl', 'rb')\n",
    "test_y = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('val_y.pkl', 'rb')\n",
    "val_y = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write To csv File\n",
    "def file_writer(file_name,row_):\n",
    "    with open(file_name, 'a') as f:\n",
    "        spamwriter = csv.writer(f, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        spamwriter.writerow(row_ )\n",
    "        \n",
    "#Batching\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Possible Hyperparameter List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size_list = [16,16,32,64]\n",
    "lstm_layers_list = [1]\n",
    "batch_size_list = [128,32,64,128]\n",
    "n_epochs_list = [10,15,20]\n",
    "\n",
    "all_hyperpara_list = []\n",
    "for a in lstm_size_list:\n",
    "    for b in lstm_layers_list:\n",
    "        for c in batch_size_list:\n",
    "            for d in n_epochs_list:\n",
    "                all_hyperpara_list.append((a,b,c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperpara_index = 0 #Hyperparameter S.No.\n",
    "\n",
    "for hyperameter_tuple in all_hyperpara_list: \n",
    "    \n",
    "    #Current set of Hyperparmeters\n",
    "    lstm_size = hyperameter_tuple[0]\n",
    "    lstm_layers = hyperameter_tuple[1]\n",
    "    batch_size = hyperameter_tuple[2]\n",
    "    n_epochs = hyperameter_tuple[3]\n",
    "    file_writer('hyper_'+str(hyperpara_index)+'.csv',['lstm_size= '+str(lstm_size), 'lstm_layers= '+str(lstm_layers),'batch_size= '+str(batch_size),'n_epochs= '+str(n_epochs)])    \n",
    "\n",
    "    #Reset Graph \n",
    "    from tensorflow.python.framework import ops\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    #Placeholder\n",
    "    X = tf.placeholder(tf.float32, [None, None, 300], name = 'inputs')\n",
    "    Y = tf.placeholder(tf.float32, [None, 1], name = 'labels')\n",
    "\n",
    "    #Build Network\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm]*lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, X, initial_state = initial_state)\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1],1, activation_fn=tf.tanh)\n",
    "\n",
    "    #Optimisation\n",
    "    loss = tf.reduce_mean(tf.square(Y - predictions))\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    #Accuracy\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.float32), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    #Start Time\n",
    "    start_time = time.time()\n",
    "\n",
    "    #Temp Variables\n",
    "    x_ = [] #Accounts for batch size\n",
    "    loss_count = 0\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    #START SESSION:\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #For every Epoch\n",
    "    for e in range(n_epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        batch_index = 1 #represents index of batch\n",
    "        \n",
    "        #For every batch\n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {X: x, Y: np.reshape(y,(len(y),1)), initial_state: state}\n",
    "            state, loss_,  _ = sess.run([final_state, loss, optimizer], feed_dict=feed)\n",
    "            \n",
    "            #Calculate TrainLoss & ValidationLoss for building Loss graph\n",
    "            if batch_index%50==0:\n",
    "                #Train Loss:\n",
    "                print(\"Epoch: {}/{}\".format(e, n_epochs),\"Iteration: {}\".format(batch_index),\"Train loss: {:.3f}\".format(loss_))\n",
    "                train_loss.append(loss_)\n",
    "                loss_count = loss_count + 1\n",
    "                x_.append(loss_count)\n",
    "\n",
    "                #Validation loss: \n",
    "                #Steps =  - Calculate loss for every batch in validation set\n",
    "                #         - Take the mean of all batch as final validation loss\n",
    "                temp_val_loss = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size): \n",
    "                    feed = {X: x,Y: np.reshape(y,(len(y),1)),initial_state: val_state}\n",
    "                    loss_, val_state = sess.run([loss, final_state], feed_dict=feed)\n",
    "                    temp_val_loss.append(loss_)\n",
    "                val_loss.append(np.mean(temp_val_loss))  \n",
    "                \n",
    "            #Calculate TrainAcc & ValidationAcc & TestAcc for writing in file\n",
    "            if batch_index%100==0:\n",
    "                #Validation Acc:\n",
    "                #Steps =  - Calculate loss for every batch in validation set\n",
    "                #         - Take the mean of all batch as final validation Accuracy\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {X: x,Y: np.reshape(y,(len(y),1)),initial_state: val_state}\n",
    "\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.5f}\".format(np.mean(val_acc)))\n",
    "                file_writer('hyper_'+str(hyperpara_index)+'.csv',['Epoch= '+str(e),'Validation Acc= '+str(np.mean(val_acc)) ])\n",
    "\n",
    "                #Train Acc:\n",
    "                train_acc = []\n",
    "                train_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "                    feed = {X: x,Y:  np.reshape(y,(len(y),1)),initial_state: train_state}\n",
    "\n",
    "                    batch_acc, train_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    train_acc.append(batch_acc)\n",
    "                print(\"Train accuracy: {:.5f}\".format(np.mean(train_acc)))\n",
    "                file_writer('hyper_'+str(hyperpara_index)+'.csv',['Epoch= '+str(e),'Train Acc= '+str(np.mean(train_acc)) ])\n",
    "\n",
    "                #Test Acc: \n",
    "                test_acc = []\n",
    "                test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "                    feed = {X: x,Y: np.reshape(y,(len(y),1)),initial_state: test_state}\n",
    "\n",
    "                    batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    test_acc.append(batch_acc)\n",
    "                print(\"Test accuracy: {:.5f}\".format(np.mean(test_acc)))\n",
    "                file_writer('hyper_'+str(hyperpara_index)+'.csv',['Epoch= '+str(e),'Test Acc= '+str(np.mean(test_acc)) ])\n",
    "\n",
    "            #represents index of batch\n",
    "            batch_index +=1\n",
    "    \n",
    "    #All epochs completed\n",
    "    print('Training Completed')\n",
    "\n",
    "    #Test Acc(after complete training): \n",
    "    test_acc = []\n",
    "    #test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {X: x,Y: np.reshape(y,(len(y),1))}\n",
    "\n",
    "        batch_acc = sess.run([accuracy], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.5f}\".format(np.mean(test_acc)))\n",
    "    file_writer('hyper_'+str(hyperpara_index)+'.csv',['Epoch= '+str(e),'Final Test Acc= '+str(np.mean(test_acc)) ])\n",
    "\n",
    "    #Total Time Taken for this hyperparameter set\n",
    "    time_taken = time.time()-start_time \n",
    "\n",
    "    #Build Graph:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(x_,train_loss,'g',x_,val_loss,'b')\n",
    "    plt.savefig(\"figure\"+str(hyperpara_index)+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "    #Write Time_Taken, Min_Val_loss\n",
    "    file_writer('hyper_'+str(hyperpara_index)+'.csv',['time_taken= '+str(time_taken),'min val loss= '+str(np.min(val_loss)) ])\n",
    "\n",
    "    #Update Hyperparameter S.No.\n",
    "    hyperpara_index+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
